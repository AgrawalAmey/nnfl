@ARTICLE{ntm,
   author = {{Graves}, A. and {Wayne}, G. and {Danihelka}, I.},
    title = "{Neural Turing Machines}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1410.5401},
 keywords = {Computer Science - Neural and Evolutionary Computing},
     year = 2014,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1410.5401G},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{dnc,
  author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwińska, Agnieszka and Colmenarejo, Sergio Gómez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adrià Puigdomènech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
  year = {2016},
  title = {Hybrid computing using a neural network with dynamic external memory},
  journal = {Nature},
  publisher = {Nature Publishing Group},
  issn = {0028-0836},
  doi = {10.1038/nature20101},
  url = {http:https://dx.doi.org/10.1038/nature20101},
  volume = {538},
  month = {10},
  pages = {471--476},
  number = {7626},
  abstract = {Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read–write memory.}
}

@ARTICLE{seq2seq,
   author = {{Sutskever}, I. and {Vinyals}, O. and {Le}, Q.~V.},
    title = "{Sequence to Sequence Learning with Neural Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1409.3215},
 primaryClass = "cs.CL",
 keywords = {Computer Science - Computation and Language, Computer Science - Learning},
     year = 2014,
    month = sep,
   adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1409.3215S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{ptr-net,
   author = {{Vinyals}, O. and {Fortunato}, M. and {Jaitly}, N.},
    title = "{Pointer Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1506.03134},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Computational Geometry, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
     year = 2015,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150603134V},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{nco,
   author = {{Bello}, I. and {Pham}, H. and {Le}, Q.~V. and {Norouzi}, M. and
	{Bengio}, S.},
    title = "{Neural Combinatorial Optimization with Reinforcement Learning}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1611.09940},
 primaryClass = "cs.AI",
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
     year = 2016,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161109940B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{dntm,
   author = {{Gulcehre}, C. and {Chandar}, S. and {Cho}, K. and {Bengio}, Y.
	},
    title = "{Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1607.00036},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
     year = 2016,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160700036G},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{ngpu,
   author = {{Kaiser}, {\L}. and {Sutskever}, I.},
    title = "{Neural GPUs Learn Algorithms}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1511.08228},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
     year = 2015,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151108228K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016arXiv161100736P,
   author = {{Price}, E. and {Zaremba}, W. and {Sutskever}, I.},
    title = "{Extensions and Limitations of the Neural GPU}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1611.00736},
 keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
     year = 2016,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161100736P},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015arXiv150500521Z,
   author = {{Zaremba}, W. and {Sutskever}, I.},
    title = "{Reinforcement Learning Neural Turing Machines - Revised}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1505.00521},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2015,
    month = may,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150500521Z},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016arXiv161202336A,
   author = {{Ale{\v s}}, J.},
    title = "{Neural Turing Machines: Convergence of Copy Tasks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1612.02336},
 keywords = {Computer Science - Neural and Evolutionary Computing},
     year = 2016,
    month = dec,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161202336A},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015arXiv151003931Z,
   author = {{Zhang}, W. and {Yu}, Y. and {Zhou}, B.},
    title = "{Structured Memory for Neural Turing Machines}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1510.03931},
 primaryClass = "cs.AI",
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, I.2.6},
     year = 2015,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151003931Z},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015SciA....1E0031T,
   author = {{Traversa}, F.~L. and {Ramella}, C. and {Bonani}, F. and {Di Ventra}, M.
	},
    title = "{Memcomputing NP-complete problems in polynomial time using polynomial resources and collective states}",
  journal = {Science Advances},
archivePrefix = "arXiv",
   eprint = {1411.4798},
     year = 2015,
    month = jul,
   volume = 1,
    pages = {e1500031-e1500031},
      doi = {10.1126/sciadv.1500031},
   adsurl = {http://adsabs.harvard.edu/abs/2015SciA....1E0031T},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{bengio94,
    author = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
     title = {Learning Long-Term Dependencies with Gradient Descent is Difficult},
   journal = {IEEE Transactions on Neural Networks},
    volume = {5},
    number = {2},
      year = {1994},
     pages = {157--166},
       url = {http://www.iro.umontreal.ca/~lisa/pointeurs/ieeetrnn94.pdf},
  abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captures increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
optnote={(Special Issue on Recurrent Neural Networks)},topics={LongTerm},cat={J},
}

@inproceedings{siegelmann92,
 author = {Siegelmann, Hava T. and Sontag, Eduardo D.},
 title = {On the Computational Power of Neural Nets},
 booktitle = {Proceedings of the Fifth Annual Workshop on Computational Learning Theory},
 series = {COLT '92},
 year = {1992},
 isbn = {0-89791-497-X},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {440--449},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/130385.130432},
 doi = {10.1145/130385.130432},
 acmid = {130432},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{lstm97,
 author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
 title = {Long Short-Term Memory},
 journal = {Neural Comput.},
 issue_date = {November 15, 1997},
 volume = {9},
 number = {8},
 month = nov,
 year = {1997},
 issn = {0899-7667},
 pages = {1735--1780},
 numpages = {46},
 url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
 doi = {10.1162/neco.1997.9.8.1735},
 acmid = {1246450},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}

@inproceedings{graves2014,
  title={Towards End-To-End Speech Recognition with Recurrent Neural Networks.},
  author={Graves, Alex and Jaitly, Navdeep},
  booktitle={ICML},
  volume={14},
  pages={1764--1772},
  year={2014}
}

@inproceedings{sutskever2011,
  title={Generating text with recurrent neural networks},
  author={Sutskever, Ilya and Martens, James and Hinton, Geoffrey E},
  booktitle={Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  pages={1017--1024},
  year={2011}
}

@article{sonderby2014protein,
  title={Protein secondary structure prediction with long short term memory networks},
  author={S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  journal={arXiv preprint arXiv:1412.7828},
  year={2014}
}

@article{bahdanau2014,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}
